<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.5" />
<title>dpm.dpgmm API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>dpm.dpgmm</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import warnings

from sklearn import cluster
from sklearn.exceptions import ConvergenceWarning
from sklearn.mixture import BayesianGaussianMixture
import numpy as np
from sklearn.mixture._base import _check_X
from logging import getLogger
from sklearn.mixture._bayesian_mixture import _log_wishart_norm, _log_dirichlet_norm
from sklearn.mixture._gaussian_mixture import _compute_log_det_cholesky
from scipy.special import betaln
from sklearn.utils import check_random_state

logger = getLogger(__name__)


class WeightedDPGMM(BayesianGaussianMixture):
    def fit_predict(self, X, y=None, sample_weight: np.ndarray = None):
        if sample_weight is None:
            logger.warning(&#34;no sample weights provided .. use unweighted model instead&#34;)
            return super().fit_predict(X, y=None)
        &#34;&#34;&#34;Estimate model parameters using X and predict the labels for X.

        The method fits the model n_init times and sets the parameters with
        which the model has the largest likelihood or lower bound. Within each
        trial, the method iterates between E-step and M-step for `max_iter`
        times until the change of likelihood or lower bound is less than
        `tol`, otherwise, a :class:`~sklearn.exceptions.ConvergenceWarning` is
        raised. After fitting, it predicts the most probable label for the
        input data points.

        .. versionadded:: 0.20

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            List of n_features-dimensional data points. Each row
            corresponds to a single data point.

        Returns
        -------
        labels : array, shape (n_samples,)
            Component labels.
        &#34;&#34;&#34;
        X = _check_X(X, self.n_components, ensure_min_samples=2)
        self._check_initial_parameters(X)

        #  check sample  weights
        if not (sample_weight &gt;= 1.).all():
            raise ValueError(&#34;sample_weight must be all greater or equal to 1&#34;)

        log_sample_weight = np.log(sample_weight[:, np.newaxis])

        # if we enable warm_start, we will have a unique initialisation
        do_init = not (self.warm_start and hasattr(self, &#39;converged_&#39;))
        n_init = self.n_init if do_init else 1

        max_lower_bound = -np.infty
        self.converged_ = False

        random_state = check_random_state(self.random_state)

        n_samples, _ = X.shape

        for init in range(n_init):
            self._print_verbose_msg_init_beg(init)

            if do_init:
                self._initialize_parameters(X, random_state, sample_weight)

            lower_bound = (-np.infty if do_init else self.lower_bound_)

            for n_iter in range(1, self.max_iter + 1):
                prev_lower_bound = lower_bound

                log_prob_norm, log_resp = self._e_step(X)
                self._m_step(X, log_resp + log_sample_weight)
                lower_bound = self._compute_lower_bound(
                    log_resp, log_prob_norm, sample_weight)

                change = lower_bound - prev_lower_bound
                self._print_verbose_msg_iter_end(n_iter, change)

                if abs(change) &lt; self.tol:
                    self.converged_ = True
                    break

            self._print_verbose_msg_init_end(lower_bound)

            if lower_bound &gt; max_lower_bound:
                max_lower_bound = lower_bound
                best_params = self._get_parameters()
                best_n_iter = n_iter

        if not self.converged_:
            warnings.warn(&#39;Initialization %d did not converge. &#39;
                          &#39;Try different init parameters, &#39;
                          &#39;or increase max_iter, tol &#39;
                          &#39;or check for degenerate data.&#39;
                          % (init + 1), ConvergenceWarning)

        self._set_parameters(best_params)
        self.n_iter_ = best_n_iter
        self.lower_bound_ = max_lower_bound

        # Always do a final e-step to guarantee that the labels returned by
        # fit_predict(X) are always consistent with fit(X).predict(X)
        # for any value of max_iter and tol (and any random_state).
        _, log_resp = self._e_step(X)

        return log_resp.argmax(axis=1)

    def _initialize_parameters(self, X, random_state, sample_weight):
        &#34;&#34;&#34;Initialize the model parameters.

        Parameters
        ----------
        X : array-like, shape  (n_samples, n_features)

        random_state : RandomState
            A random number generator instance.
        &#34;&#34;&#34;
        n_samples, _ = X.shape

        if self.init_params == &#39;kmeans&#39;:
            resp = np.zeros((n_samples, self.n_components))
            label = cluster.KMeans(n_clusters=self.n_components, n_init=1,
                                   random_state=random_state).fit(X, sample_weight=sample_weight).labels_
            resp[np.arange(n_samples), label] = 1
        elif self.init_params == &#39;random&#39;:
            resp = random_state.rand(n_samples, self.n_components)
            resp /= resp.sum(axis=1)[:, np.newaxis]
        else:
            raise ValueError(&#34;Unimplemented initialization method &#39;%s&#39;&#34;
                             % self.init_params)

        # multiple resp by sample weight
        resp = sample_weight[:, np.newaxis] * resp

        self._initialize(X, resp)

    def _compute_lower_bound(self, log_resp, log_prob_norm, counts):
        &#34;&#34;&#34;Estimate the lower bound of the model.

        The lower bound on the likelihood (of the training data with respect to
        the model) is used to detect the convergence and has to decrease at
        each iteration.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)

        log_resp : array, shape (n_samples, n_components)
            Logarithm of the posterior probabilities (or responsibilities) of
            the point of each sample in X.

        log_prob_norm : float
            Logarithm of the probability of each sample in X.

        Returns
        -------
        lower_bound : float
        &#34;&#34;&#34;
        # Contrary to the original formula, we have done some simplification
        # and removed all the constant terms.
        n_features, = self.mean_prior_.shape

        # We removed `.5 * n_features * np.log(self.degrees_of_freedom_)`
        # because the precision matrix is normalized.
        log_det_precisions_chol = (_compute_log_det_cholesky(
            self.precisions_cholesky_, self.covariance_type, n_features) -
                                   .5 * n_features * np.log(self.degrees_of_freedom_))

        if self.covariance_type == &#39;tied&#39;:
            log_wishart = self.n_components * np.float64(_log_wishart_norm(
                self.degrees_of_freedom_, log_det_precisions_chol, n_features))
        else:
            log_wishart = np.sum(_log_wishart_norm(
                self.degrees_of_freedom_, log_det_precisions_chol, n_features))

        if self.weight_concentration_prior_type == &#39;dirichlet_process&#39;:
            log_norm_weight = -np.sum(betaln(self.weight_concentration_[0],
                                             self.weight_concentration_[1]))
        else:
            log_norm_weight = _log_dirichlet_norm(self.weight_concentration_)

        H_resp = (np.exp(log_resp) * log_resp).sum(1).dot(counts)

        return (-H_resp -
                log_wishart - log_norm_weight -
                0.5 * n_features * np.sum(np.log(self.mean_precision_)))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="dpm.dpgmm.WeightedDPGMM"><code class="flex name class">
<span>class <span class="ident">WeightedDPGMM</span></span>
<span>(</span><span>n_components=1, covariance_type='full', tol=0.001, reg_covar=1e-06, max_iter=100, n_init=1, init_params='kmeans', weight_concentration_prior_type='dirichlet_process', weight_concentration_prior=None, mean_precision_prior=None, mean_prior=None, degrees_of_freedom_prior=None, covariance_prior=None, random_state=None, warm_start=False, verbose=0, verbose_interval=10)</span>
</code></dt>
<dd>
<section class="desc"><p>Variational Bayesian estimation of a Gaussian mixture.</p>
<p>This class allows to infer an approximate posterior distribution over the
parameters of a Gaussian mixture distribution. The effective number of
components can be inferred from the data.</p>
<p>This class implements two types of prior for the weights distribution: a
finite mixture model with Dirichlet distribution and an infinite mixture
model with the Dirichlet Process. In practice Dirichlet Process inference
algorithm is approximated and uses a truncated distribution with a fixed
maximum number of components (called the Stick-breaking representation).
The number of components actually used almost always depends on the data.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;0.18</p>
</div>
<p>Read more in the :ref:<code>User Guide &lt;bgmm&gt;</code>.</p>
<h2 id="parameters">Parameters</h2>
<p>n_components : int, defaults to 1.
The number of mixture components. Depending on the data and the value
of the <code>weight_concentration_prior</code> the model can decide to not use
all the components by setting some component <code>weights_</code> to values very
close to zero. The number of effective components is therefore smaller
than n_components.</p>
<dl>
<dt><strong><code>covariance_type</code></strong> :&ensp;{<code>'full'</code>, <code>'tied'</code>, <code>'diag'</code>, <code>'spherical'</code>}, <code>defaults</code> <code>to</code> <code>'full'</code></dt>
<dd>String describing the type of covariance parameters to use.
Must be one of::<pre><code>'full' (each component has its own general covariance matrix),
'tied' (all components share the same general covariance matrix),
'diag' (each component has its own diagonal covariance matrix),
'spherical' (each component has its own single variance).
</code></pre>
</dd>
</dl>
<p>tol : float, defaults to 1e-3.
The convergence threshold. EM iterations will stop when the
lower bound average gain on the likelihood (of the training data with
respect to the model) is below this threshold.</p>
<p>reg_covar : float, defaults to 1e-6.
Non-negative regularization added to the diagonal of covariance.
Allows to assure that the covariance matrices are all positive.</p>
<p>max_iter : int, defaults to 100.
The number of EM iterations to perform.</p>
<p>n_init : int, defaults to 1.
The number of initializations to perform. The result with the highest
lower bound value on the likelihood is kept.</p>
<p>init_params : {'kmeans', 'random'}, defaults to 'kmeans'.
The method used to initialize the weights, the means and the
covariances.
Must be one of::</p>
<pre><code>    'kmeans' : responsibilities are initialized using kmeans.
    'random' : responsibilities are initialized randomly.
</code></pre>
<p>weight_concentration_prior_type : str, defaults to 'dirichlet_process'.
String describing the type of the weight concentration prior.
Must be one of::</p>
<pre><code>    'dirichlet_process' (using the Stick-breaking representation),
    'dirichlet_distribution' (can favor more uniform weights).
</code></pre>
<p>weight_concentration_prior : float | None, optional.
The dirichlet concentration of each component on the weight
distribution (Dirichlet). This is commonly called gamma in the
literature. The higher concentration puts more mass in
the center and will lead to more components being active, while a lower
concentration parameter will lead to more mass at the edge of the
mixture weights simplex. The value of the parameter must be greater
than 0. If it is None, it's set to <code>1. / n_components</code>.</p>
<p>mean_precision_prior : float | None, optional.
The precision prior on the mean distribution (Gaussian).
Controls the extent of where means can be placed. Larger
values concentrate the cluster means around <code>mean_prior</code>.
The value of the parameter must be greater than 0.
If it is None, it is set to 1.</p>
<dl>
<dt><strong><code>mean_prior</code></strong> :&ensp;<code>array</code>-<code>like</code>, <code>shape</code> (<code>n_features</code>,), optional</dt>
<dd>The prior on the mean distribution (Gaussian).
If it is None, it is set to the mean of X.</dd>
</dl>
<p>degrees_of_freedom_prior : float | None, optional.
The prior of the number of degrees of freedom on the covariance
distributions (Wishart). If it is None, it's set to <code>n_features</code>.</p>
<dl>
<dt><strong><code>covariance_prior</code></strong> :&ensp;<code>float</code> or <code>array</code>-<code>like</code>, optional</dt>
<dd>The prior on the covariance distribution (Wishart).
If it is None, the emiprical covariance prior is initialized using the
covariance of X. The shape depends on <code>covariance_type</code>::<pre><code>    (n_features, n_features) if 'full',
    (n_features, n_features) if 'tied',
    (n_features)             if 'diag',
    float                    if 'spherical'
</code></pre>
</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code>, <code>RandomState</code> <code>instance</code> or <code>None</code>, optional (default=<code>None</code>)</dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <code>np.random</code>.</dd>
</dl>
<p>warm_start : bool, default to False.
If 'warm_start' is True, the solution of the last fitting is used as
initialization for the next call of fit(). This can speed up
convergence when fit is called several times on similar problems.
See :term:<code>the Glossary &lt;warm_start&gt;</code>.</p>
<p>verbose : int, default to 0.
Enable verbose output. If 1 then it prints the current
initialization and each iteration step. If greater than 1 then
it prints also the log probability and the time needed
for each step.</p>
<p>verbose_interval : int, default to 10.
Number of iteration done before the next print.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>weights_</code></strong> :&ensp;<code>array</code>-<code>like</code>, <code>shape</code> (<code>n_components</code>,)</dt>
<dd>The weights of each mixture components.</dd>
<dt><strong><code>means_</code></strong> :&ensp;<code>array</code>-<code>like</code>, <code>shape</code> (<code>n_components</code>, <code>n_features</code>)</dt>
<dd>The mean of each mixture component.</dd>
<dt><strong><code>covariances_</code></strong> :&ensp;<code>array</code>-<code>like</code></dt>
<dd>The covariance of each mixture component.
The shape depends on <code>covariance_type</code>::<pre><code>(n_components,)                        if 'spherical',
(n_features, n_features)               if 'tied',
(n_components, n_features)             if 'diag',
(n_components, n_features, n_features) if 'full'
</code></pre>
</dd>
<dt><strong><code>precisions_</code></strong> :&ensp;<code>array</code>-<code>like</code></dt>
<dd>The precision matrices for each component in the mixture. A precision
matrix is the inverse of a covariance matrix. A covariance matrix is
symmetric positive definite so the mixture of Gaussian can be
equivalently parameterized by the precision matrices. Storing the
precision matrices instead of the covariance matrices makes it more
efficient to compute the log-likelihood of new samples at test time.
The shape depends on <code>covariance_type</code>::<pre><code>(n_components,)                        if 'spherical',
(n_features, n_features)               if 'tied',
(n_components, n_features)             if 'diag',
(n_components, n_features, n_features) if 'full'
</code></pre>
</dd>
<dt><strong><code>precisions_cholesky_</code></strong> :&ensp;<code>array</code>-<code>like</code></dt>
<dd>The cholesky decomposition of the precision matrices of each mixture
component. A precision matrix is the inverse of a covariance matrix.
A covariance matrix is symmetric positive definite so the mixture of
Gaussian can be equivalently parameterized by the precision matrices.
Storing the precision matrices instead of the covariance matrices makes
it more efficient to compute the log-likelihood of new samples at test
time. The shape depends on <code>covariance_type</code>::<pre><code>(n_components,)                        if 'spherical',
(n_features, n_features)               if 'tied',
(n_components, n_features)             if 'diag',
(n_components, n_features, n_features) if 'full'
</code></pre>
</dd>
<dt><strong><code>converged_</code></strong> :&ensp;<code>bool</code></dt>
<dd>True when convergence was reached in fit(), False otherwise.</dd>
<dt><strong><code>n_iter_</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of step used by the best fit of inference to reach the
convergence.</dd>
<dt><strong><code>lower_bound_</code></strong> :&ensp;<code>float</code></dt>
<dd>Lower bound value on the likelihood (of the training data with
respect to the model) of the best fit of inference.</dd>
<dt><strong><code>weight_concentration_prior_</code></strong> :&ensp;<code>tuple</code> or <code>float</code></dt>
<dd>
<p>The dirichlet concentration of each component on the weight
distribution (Dirichlet). The type depends on
<code>weight_concentration_prior_type</code>::</p>
<pre><code>(float, float) if 'dirichlet_process' (Beta parameters),
float          if 'dirichlet_distribution' (Dirichlet parameters).
</code></pre>
<p>The higher concentration puts more mass in
the center and will lead to more components being active, while a lower
concentration parameter will lead to more mass at the edge of the
simplex.</p>
</dd>
<dt><strong><code>weight_concentration_</code></strong> :&ensp;<code>array</code>-<code>like</code>, <code>shape</code> (<code>n_components</code>,)</dt>
<dd>The dirichlet concentration of each component on the weight
distribution (Dirichlet).</dd>
<dt><strong><code>mean_precision_prior_</code></strong> :&ensp;<code>float</code></dt>
<dd>The precision prior on the mean distribution (Gaussian).
Controls the extent of where means can be placed.
Larger values concentrate the cluster means around <code>mean_prior</code>.
If mean_precision_prior is set to None, <code>mean_precision_prior_</code> is set
to 1.</dd>
<dt><strong><code>mean_precision_</code></strong> :&ensp;<code>array</code>-<code>like</code>, <code>shape</code> (<code>n_components</code>,)</dt>
<dd>The precision of each components on the mean distribution (Gaussian).</dd>
<dt><strong><code>mean_prior_</code></strong> :&ensp;<code>array</code>-<code>like</code>, <code>shape</code> (<code>n_features</code>,)</dt>
<dd>The prior on the mean distribution (Gaussian).</dd>
<dt><strong><code>degrees_of_freedom_prior_</code></strong> :&ensp;<code>float</code></dt>
<dd>The prior of the number of degrees of freedom on the covariance
distributions (Wishart).</dd>
<dt><strong><code>degrees_of_freedom_</code></strong> :&ensp;<code>array</code>-<code>like</code>, <code>shape</code> (<code>n_components</code>,)</dt>
<dd>The number of degrees of freedom of each components in the model.</dd>
<dt><strong><code>covariance_prior_</code></strong> :&ensp;<code>float</code> or <code>array</code>-<code>like</code></dt>
<dd>The prior on the covariance distribution (Wishart).
The shape depends on <code>covariance_type</code>::<pre><code>(n_features, n_features) if 'full',
(n_features, n_features) if 'tied',
(n_features)             if 'diag',
float                    if 'spherical'
</code></pre>
</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>GaussianMixture</code></dt>
<dd>Finite Gaussian mixture fit with EM.</dd>
</dl>
<h2 id="references">References</h2>
<p>.. [1] <code>Bishop, Christopher M. (2006). "Pattern recognition and machine
learning". Vol. 4 No. 4. New York: Springer.
&lt;https://www.springer.com/kr/book/9780387310732&gt;</code>_</p>
<p>.. [2] <code>Hagai Attias. (2000). "A Variational Bayesian Framework for
Graphical Models". In Advances in Neural Information Processing
Systems 12.
&lt;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.36.2841&amp;rep=rep1&amp;type=pdf&gt;</code>_</p>
<p>.. [3] <code>Blei, David M. and Michael I. Jordan. (2006). "Variational
inference for Dirichlet process mixtures". Bayesian analysis 1.1
&lt;https://www.cs.princeton.edu/courses/archive/fall11/cos597C/reading/BleiJordan2005.pdf&gt;</code>_</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class WeightedDPGMM(BayesianGaussianMixture):
    def fit_predict(self, X, y=None, sample_weight: np.ndarray = None):
        if sample_weight is None:
            logger.warning(&#34;no sample weights provided .. use unweighted model instead&#34;)
            return super().fit_predict(X, y=None)
        &#34;&#34;&#34;Estimate model parameters using X and predict the labels for X.

        The method fits the model n_init times and sets the parameters with
        which the model has the largest likelihood or lower bound. Within each
        trial, the method iterates between E-step and M-step for `max_iter`
        times until the change of likelihood or lower bound is less than
        `tol`, otherwise, a :class:`~sklearn.exceptions.ConvergenceWarning` is
        raised. After fitting, it predicts the most probable label for the
        input data points.

        .. versionadded:: 0.20

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            List of n_features-dimensional data points. Each row
            corresponds to a single data point.

        Returns
        -------
        labels : array, shape (n_samples,)
            Component labels.
        &#34;&#34;&#34;
        X = _check_X(X, self.n_components, ensure_min_samples=2)
        self._check_initial_parameters(X)

        #  check sample  weights
        if not (sample_weight &gt;= 1.).all():
            raise ValueError(&#34;sample_weight must be all greater or equal to 1&#34;)

        log_sample_weight = np.log(sample_weight[:, np.newaxis])

        # if we enable warm_start, we will have a unique initialisation
        do_init = not (self.warm_start and hasattr(self, &#39;converged_&#39;))
        n_init = self.n_init if do_init else 1

        max_lower_bound = -np.infty
        self.converged_ = False

        random_state = check_random_state(self.random_state)

        n_samples, _ = X.shape

        for init in range(n_init):
            self._print_verbose_msg_init_beg(init)

            if do_init:
                self._initialize_parameters(X, random_state, sample_weight)

            lower_bound = (-np.infty if do_init else self.lower_bound_)

            for n_iter in range(1, self.max_iter + 1):
                prev_lower_bound = lower_bound

                log_prob_norm, log_resp = self._e_step(X)
                self._m_step(X, log_resp + log_sample_weight)
                lower_bound = self._compute_lower_bound(
                    log_resp, log_prob_norm, sample_weight)

                change = lower_bound - prev_lower_bound
                self._print_verbose_msg_iter_end(n_iter, change)

                if abs(change) &lt; self.tol:
                    self.converged_ = True
                    break

            self._print_verbose_msg_init_end(lower_bound)

            if lower_bound &gt; max_lower_bound:
                max_lower_bound = lower_bound
                best_params = self._get_parameters()
                best_n_iter = n_iter

        if not self.converged_:
            warnings.warn(&#39;Initialization %d did not converge. &#39;
                          &#39;Try different init parameters, &#39;
                          &#39;or increase max_iter, tol &#39;
                          &#39;or check for degenerate data.&#39;
                          % (init + 1), ConvergenceWarning)

        self._set_parameters(best_params)
        self.n_iter_ = best_n_iter
        self.lower_bound_ = max_lower_bound

        # Always do a final e-step to guarantee that the labels returned by
        # fit_predict(X) are always consistent with fit(X).predict(X)
        # for any value of max_iter and tol (and any random_state).
        _, log_resp = self._e_step(X)

        return log_resp.argmax(axis=1)

    def _initialize_parameters(self, X, random_state, sample_weight):
        &#34;&#34;&#34;Initialize the model parameters.

        Parameters
        ----------
        X : array-like, shape  (n_samples, n_features)

        random_state : RandomState
            A random number generator instance.
        &#34;&#34;&#34;
        n_samples, _ = X.shape

        if self.init_params == &#39;kmeans&#39;:
            resp = np.zeros((n_samples, self.n_components))
            label = cluster.KMeans(n_clusters=self.n_components, n_init=1,
                                   random_state=random_state).fit(X, sample_weight=sample_weight).labels_
            resp[np.arange(n_samples), label] = 1
        elif self.init_params == &#39;random&#39;:
            resp = random_state.rand(n_samples, self.n_components)
            resp /= resp.sum(axis=1)[:, np.newaxis]
        else:
            raise ValueError(&#34;Unimplemented initialization method &#39;%s&#39;&#34;
                             % self.init_params)

        # multiple resp by sample weight
        resp = sample_weight[:, np.newaxis] * resp

        self._initialize(X, resp)

    def _compute_lower_bound(self, log_resp, log_prob_norm, counts):
        &#34;&#34;&#34;Estimate the lower bound of the model.

        The lower bound on the likelihood (of the training data with respect to
        the model) is used to detect the convergence and has to decrease at
        each iteration.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)

        log_resp : array, shape (n_samples, n_components)
            Logarithm of the posterior probabilities (or responsibilities) of
            the point of each sample in X.

        log_prob_norm : float
            Logarithm of the probability of each sample in X.

        Returns
        -------
        lower_bound : float
        &#34;&#34;&#34;
        # Contrary to the original formula, we have done some simplification
        # and removed all the constant terms.
        n_features, = self.mean_prior_.shape

        # We removed `.5 * n_features * np.log(self.degrees_of_freedom_)`
        # because the precision matrix is normalized.
        log_det_precisions_chol = (_compute_log_det_cholesky(
            self.precisions_cholesky_, self.covariance_type, n_features) -
                                   .5 * n_features * np.log(self.degrees_of_freedom_))

        if self.covariance_type == &#39;tied&#39;:
            log_wishart = self.n_components * np.float64(_log_wishart_norm(
                self.degrees_of_freedom_, log_det_precisions_chol, n_features))
        else:
            log_wishart = np.sum(_log_wishart_norm(
                self.degrees_of_freedom_, log_det_precisions_chol, n_features))

        if self.weight_concentration_prior_type == &#39;dirichlet_process&#39;:
            log_norm_weight = -np.sum(betaln(self.weight_concentration_[0],
                                             self.weight_concentration_[1]))
        else:
            log_norm_weight = _log_dirichlet_norm(self.weight_concentration_)

        H_resp = (np.exp(log_resp) * log_resp).sum(1).dot(counts)

        return (-H_resp -
                log_wishart - log_norm_weight -
                0.5 * n_features * np.sum(np.log(self.mean_precision_)))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.mixture._bayesian_mixture.BayesianGaussianMixture</li>
<li>sklearn.mixture._base.BaseMixture</li>
<li>sklearn.base.DensityMixin</li>
<li>sklearn.base.BaseEstimator</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="dpm.dpgmm.WeightedDPGMM.fit_predict"><code class="name flex">
<span>def <span class="ident">fit_predict</span></span>(<span>self, X, y=None, sample_weight=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Estimate model parameters using X and predict the labels for X.</p>
<p>The method fits the model n_init times and sets the parameters with
which the model has the largest likelihood or lower bound. Within each
trial, the method iterates between E-step and M-step for <code>max_iter</code>
times until the change of likelihood or lower bound is less than
<code>tol</code>, otherwise, a :class:<code>~sklearn.exceptions.ConvergenceWarning</code> is
raised. After fitting, it predicts the most probable label for the
input data points.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;0.20</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array</code>-<code>like</code>, <code>shape</code> (<code>n_samples</code>, <code>n_features</code>)</dt>
<dd>List of n_features-dimensional data points. Each row
corresponds to a single data point.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>labels</code></strong> :&ensp;<code>array</code>, <code>shape</code> (<code>n_samples</code>,)</dt>
<dd>Component labels.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit_predict(self, X, y=None, sample_weight: np.ndarray = None):
    if sample_weight is None:
        logger.warning(&#34;no sample weights provided .. use unweighted model instead&#34;)
        return super().fit_predict(X, y=None)
    &#34;&#34;&#34;Estimate model parameters using X and predict the labels for X.

    The method fits the model n_init times and sets the parameters with
    which the model has the largest likelihood or lower bound. Within each
    trial, the method iterates between E-step and M-step for `max_iter`
    times until the change of likelihood or lower bound is less than
    `tol`, otherwise, a :class:`~sklearn.exceptions.ConvergenceWarning` is
    raised. After fitting, it predicts the most probable label for the
    input data points.

    .. versionadded:: 0.20

    Parameters
    ----------
    X : array-like, shape (n_samples, n_features)
        List of n_features-dimensional data points. Each row
        corresponds to a single data point.

    Returns
    -------
    labels : array, shape (n_samples,)
        Component labels.
    &#34;&#34;&#34;
    X = _check_X(X, self.n_components, ensure_min_samples=2)
    self._check_initial_parameters(X)

    #  check sample  weights
    if not (sample_weight &gt;= 1.).all():
        raise ValueError(&#34;sample_weight must be all greater or equal to 1&#34;)

    log_sample_weight = np.log(sample_weight[:, np.newaxis])

    # if we enable warm_start, we will have a unique initialisation
    do_init = not (self.warm_start and hasattr(self, &#39;converged_&#39;))
    n_init = self.n_init if do_init else 1

    max_lower_bound = -np.infty
    self.converged_ = False

    random_state = check_random_state(self.random_state)

    n_samples, _ = X.shape

    for init in range(n_init):
        self._print_verbose_msg_init_beg(init)

        if do_init:
            self._initialize_parameters(X, random_state, sample_weight)

        lower_bound = (-np.infty if do_init else self.lower_bound_)

        for n_iter in range(1, self.max_iter + 1):
            prev_lower_bound = lower_bound

            log_prob_norm, log_resp = self._e_step(X)
            self._m_step(X, log_resp + log_sample_weight)
            lower_bound = self._compute_lower_bound(
                log_resp, log_prob_norm, sample_weight)

            change = lower_bound - prev_lower_bound
            self._print_verbose_msg_iter_end(n_iter, change)

            if abs(change) &lt; self.tol:
                self.converged_ = True
                break

        self._print_verbose_msg_init_end(lower_bound)

        if lower_bound &gt; max_lower_bound:
            max_lower_bound = lower_bound
            best_params = self._get_parameters()
            best_n_iter = n_iter

    if not self.converged_:
        warnings.warn(&#39;Initialization %d did not converge. &#39;
                      &#39;Try different init parameters, &#39;
                      &#39;or increase max_iter, tol &#39;
                      &#39;or check for degenerate data.&#39;
                      % (init + 1), ConvergenceWarning)

    self._set_parameters(best_params)
    self.n_iter_ = best_n_iter
    self.lower_bound_ = max_lower_bound

    # Always do a final e-step to guarantee that the labels returned by
    # fit_predict(X) are always consistent with fit(X).predict(X)
    # for any value of max_iter and tol (and any random_state).
    _, log_resp = self._e_step(X)

    return log_resp.argmax(axis=1)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="dpm" href="index.html">dpm</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="dpm.dpgmm.WeightedDPGMM" href="#dpm.dpgmm.WeightedDPGMM">WeightedDPGMM</a></code></h4>
<ul class="">
<li><code><a title="dpm.dpgmm.WeightedDPGMM.fit_predict" href="#dpm.dpgmm.WeightedDPGMM.fit_predict">fit_predict</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.5</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>